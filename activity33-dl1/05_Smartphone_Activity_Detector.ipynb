{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"testgv","language":"python","name":"testgv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"05_Smartphone_Activity_Detector.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"zQQvTo8EIhpl"},"source":["# Neural Network Smartphone Activity Detector\n","\n","In this activity, you will train a neural network to use smartphone data to predict the activity of the user. \n","\n","This dataset has already been separated into input features and target activities. Additional information on the dataset can be found here. \n","\n","http://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions"]},{"cell_type":"markdown","metadata":{"id":"2_n0jxmCIhpo"},"source":["### Data Pre-Processing\n","\n","Prepare the data for the neural network. This includes splitting the data into a training and testing dataset, Scaling the data, and encoding the categorical target values"]},{"cell_type":"code","metadata":{"id":"2zvnatAwIhpp","executionInfo":{"status":"ok","timestamp":1621016950431,"user_tz":420,"elapsed":3072,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}}},"source":["from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"id":"zUaRg75GIhpp","executionInfo":{"status":"ok","timestamp":1621017236382,"user_tz":420,"elapsed":1863,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}},"outputId":"48cb12e4-2a5b-48d7-9762-a09bfebff775"},"source":["# Read the input features into `X`\n","X = pd.read_csv(Path(\"features.csv\"), header=None)\n","X.head()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>...</th>\n","      <th>521</th>\n","      <th>522</th>\n","      <th>523</th>\n","      <th>524</th>\n","      <th>525</th>\n","      <th>526</th>\n","      <th>527</th>\n","      <th>528</th>\n","      <th>529</th>\n","      <th>530</th>\n","      <th>531</th>\n","      <th>532</th>\n","      <th>533</th>\n","      <th>534</th>\n","      <th>535</th>\n","      <th>536</th>\n","      <th>537</th>\n","      <th>538</th>\n","      <th>539</th>\n","      <th>540</th>\n","      <th>541</th>\n","      <th>542</th>\n","      <th>543</th>\n","      <th>544</th>\n","      <th>545</th>\n","      <th>546</th>\n","      <th>547</th>\n","      <th>548</th>\n","      <th>549</th>\n","      <th>550</th>\n","      <th>551</th>\n","      <th>552</th>\n","      <th>553</th>\n","      <th>554</th>\n","      <th>555</th>\n","      <th>556</th>\n","      <th>557</th>\n","      <th>558</th>\n","      <th>559</th>\n","      <th>560</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.043580</td>\n","      <td>-0.005970</td>\n","      <td>-0.035054</td>\n","      <td>-0.995381</td>\n","      <td>-0.988366</td>\n","      <td>-0.937382</td>\n","      <td>-0.995007</td>\n","      <td>-0.988816</td>\n","      <td>-0.953325</td>\n","      <td>-0.794796</td>\n","      <td>-0.744893</td>\n","      <td>-0.648447</td>\n","      <td>0.841796</td>\n","      <td>0.708440</td>\n","      <td>0.651716</td>\n","      <td>-0.975752</td>\n","      <td>-0.999950</td>\n","      <td>-0.999888</td>\n","      <td>-0.998014</td>\n","      <td>-0.993999</td>\n","      <td>-0.991980</td>\n","      <td>-0.970970</td>\n","      <td>-0.547095</td>\n","      <td>-0.700974</td>\n","      <td>-0.622697</td>\n","      <td>0.921884</td>\n","      <td>-0.719483</td>\n","      <td>0.342168</td>\n","      <td>-0.161318</td>\n","      <td>0.266049</td>\n","      <td>-0.274351</td>\n","      <td>0.267205</td>\n","      <td>-0.020958</td>\n","      <td>0.382610</td>\n","      <td>-0.501748</td>\n","      <td>0.512463</td>\n","      <td>-0.206337</td>\n","      <td>0.376778</td>\n","      <td>0.435172</td>\n","      <td>0.660199</td>\n","      <td>...</td>\n","      <td>-0.999918</td>\n","      <td>-0.991736</td>\n","      <td>-1.0</td>\n","      <td>-0.936508</td>\n","      <td>0.349260</td>\n","      <td>-0.517127</td>\n","      <td>-0.801006</td>\n","      <td>-0.980135</td>\n","      <td>-0.961301</td>\n","      <td>-0.974129</td>\n","      <td>-0.956013</td>\n","      <td>-0.989894</td>\n","      <td>-0.980135</td>\n","      <td>-0.999240</td>\n","      <td>-0.992673</td>\n","      <td>-0.701291</td>\n","      <td>-1.000000</td>\n","      <td>-0.132480</td>\n","      <td>0.565697</td>\n","      <td>0.363478</td>\n","      <td>-0.991994</td>\n","      <td>-0.990877</td>\n","      <td>-0.990169</td>\n","      <td>-0.992521</td>\n","      <td>-0.991044</td>\n","      <td>-0.991994</td>\n","      <td>-0.999937</td>\n","      <td>-0.990537</td>\n","      <td>-0.871306</td>\n","      <td>-1.000000</td>\n","      <td>-0.012236</td>\n","      <td>-0.314848</td>\n","      <td>-0.713308</td>\n","      <td>-0.112754</td>\n","      <td>0.030400</td>\n","      <td>-0.464761</td>\n","      <td>-0.018446</td>\n","      <td>-0.841559</td>\n","      <td>0.179913</td>\n","      <td>-0.051718</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.039480</td>\n","      <td>-0.002131</td>\n","      <td>-0.029067</td>\n","      <td>-0.998348</td>\n","      <td>-0.982945</td>\n","      <td>-0.971273</td>\n","      <td>-0.998702</td>\n","      <td>-0.983315</td>\n","      <td>-0.974000</td>\n","      <td>-0.802537</td>\n","      <td>-0.736338</td>\n","      <td>-0.712415</td>\n","      <td>0.838758</td>\n","      <td>0.708440</td>\n","      <td>0.659340</td>\n","      <td>-0.987427</td>\n","      <td>-0.999993</td>\n","      <td>-0.999826</td>\n","      <td>-0.999411</td>\n","      <td>-0.998918</td>\n","      <td>-0.985482</td>\n","      <td>-0.973481</td>\n","      <td>-0.781973</td>\n","      <td>-0.534604</td>\n","      <td>-0.593165</td>\n","      <td>0.607435</td>\n","      <td>-0.266783</td>\n","      <td>0.275882</td>\n","      <td>0.200417</td>\n","      <td>0.131266</td>\n","      <td>-0.149017</td>\n","      <td>0.292436</td>\n","      <td>-0.192986</td>\n","      <td>0.217496</td>\n","      <td>-0.089175</td>\n","      <td>0.059909</td>\n","      <td>-0.236609</td>\n","      <td>-0.012696</td>\n","      <td>-0.072711</td>\n","      <td>0.578649</td>\n","      <td>...</td>\n","      <td>-0.999867</td>\n","      <td>-0.991506</td>\n","      <td>-1.0</td>\n","      <td>-0.841270</td>\n","      <td>0.533688</td>\n","      <td>-0.625993</td>\n","      <td>-0.898311</td>\n","      <td>-0.988296</td>\n","      <td>-0.983313</td>\n","      <td>-0.982951</td>\n","      <td>-0.987406</td>\n","      <td>-0.992134</td>\n","      <td>-0.988296</td>\n","      <td>-0.999811</td>\n","      <td>-0.993996</td>\n","      <td>-0.720683</td>\n","      <td>-0.948718</td>\n","      <td>-0.268979</td>\n","      <td>-0.364219</td>\n","      <td>-0.723724</td>\n","      <td>-0.995857</td>\n","      <td>-0.996580</td>\n","      <td>-0.995671</td>\n","      <td>-0.996939</td>\n","      <td>-0.994436</td>\n","      <td>-0.995857</td>\n","      <td>-0.999981</td>\n","      <td>-0.994623</td>\n","      <td>-1.000000</td>\n","      <td>-1.000000</td>\n","      <td>0.202804</td>\n","      <td>-0.603199</td>\n","      <td>-0.860677</td>\n","      <td>0.053477</td>\n","      <td>-0.007435</td>\n","      <td>-0.732626</td>\n","      <td>0.703511</td>\n","      <td>-0.845092</td>\n","      <td>0.180261</td>\n","      <td>-0.047436</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.039978</td>\n","      <td>-0.005153</td>\n","      <td>-0.022651</td>\n","      <td>-0.995482</td>\n","      <td>-0.977314</td>\n","      <td>-0.984760</td>\n","      <td>-0.996415</td>\n","      <td>-0.975835</td>\n","      <td>-0.985973</td>\n","      <td>-0.798477</td>\n","      <td>-0.736338</td>\n","      <td>-0.712415</td>\n","      <td>0.834002</td>\n","      <td>0.705008</td>\n","      <td>0.674551</td>\n","      <td>-0.988528</td>\n","      <td>-0.999972</td>\n","      <td>-0.999719</td>\n","      <td>-0.999803</td>\n","      <td>-0.996898</td>\n","      <td>-0.976781</td>\n","      <td>-0.986754</td>\n","      <td>-0.688176</td>\n","      <td>-0.520514</td>\n","      <td>-0.593165</td>\n","      <td>0.272262</td>\n","      <td>-0.056424</td>\n","      <td>0.322283</td>\n","      <td>-0.273292</td>\n","      <td>0.037180</td>\n","      <td>-0.133612</td>\n","      <td>0.332487</td>\n","      <td>-0.240491</td>\n","      <td>0.348733</td>\n","      <td>-0.195409</td>\n","      <td>0.229436</td>\n","      <td>-0.316816</td>\n","      <td>-0.123889</td>\n","      <td>-0.181137</td>\n","      <td>0.608219</td>\n","      <td>...</td>\n","      <td>-0.999845</td>\n","      <td>-0.987029</td>\n","      <td>-1.0</td>\n","      <td>-0.904762</td>\n","      <td>0.661975</td>\n","      <td>-0.725887</td>\n","      <td>-0.926663</td>\n","      <td>-0.989255</td>\n","      <td>-0.986019</td>\n","      <td>-0.984533</td>\n","      <td>-0.991701</td>\n","      <td>-0.995857</td>\n","      <td>-0.989255</td>\n","      <td>-0.999854</td>\n","      <td>-0.993256</td>\n","      <td>-0.736521</td>\n","      <td>-0.794872</td>\n","      <td>-0.212429</td>\n","      <td>-0.564868</td>\n","      <td>-0.874594</td>\n","      <td>-0.995034</td>\n","      <td>-0.995308</td>\n","      <td>-0.994868</td>\n","      <td>-0.996133</td>\n","      <td>-0.995863</td>\n","      <td>-0.995034</td>\n","      <td>-0.999973</td>\n","      <td>-0.993834</td>\n","      <td>-1.000000</td>\n","      <td>-0.555556</td>\n","      <td>0.440079</td>\n","      <td>-0.404427</td>\n","      <td>-0.761847</td>\n","      <td>-0.118559</td>\n","      <td>0.177899</td>\n","      <td>0.100699</td>\n","      <td>0.808529</td>\n","      <td>-0.849230</td>\n","      <td>0.180610</td>\n","      <td>-0.042271</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.039785</td>\n","      <td>-0.011809</td>\n","      <td>-0.028916</td>\n","      <td>-0.996194</td>\n","      <td>-0.988569</td>\n","      <td>-0.993256</td>\n","      <td>-0.996994</td>\n","      <td>-0.988526</td>\n","      <td>-0.993135</td>\n","      <td>-0.798477</td>\n","      <td>-0.752778</td>\n","      <td>-0.722186</td>\n","      <td>0.834002</td>\n","      <td>0.705008</td>\n","      <td>0.673208</td>\n","      <td>-0.990389</td>\n","      <td>-0.999978</td>\n","      <td>-0.999783</td>\n","      <td>-0.999815</td>\n","      <td>-0.996949</td>\n","      <td>-0.989437</td>\n","      <td>-0.992440</td>\n","      <td>-0.715103</td>\n","      <td>-0.860988</td>\n","      <td>-0.916429</td>\n","      <td>0.062816</td>\n","      <td>0.082940</td>\n","      <td>0.200566</td>\n","      <td>-0.378262</td>\n","      <td>0.090063</td>\n","      <td>-0.209264</td>\n","      <td>0.316530</td>\n","      <td>-0.090862</td>\n","      <td>0.396383</td>\n","      <td>-0.353643</td>\n","      <td>0.503754</td>\n","      <td>-0.490389</td>\n","      <td>-0.304759</td>\n","      <td>-0.362708</td>\n","      <td>0.506602</td>\n","      <td>...</td>\n","      <td>-0.999894</td>\n","      <td>-0.988427</td>\n","      <td>-1.0</td>\n","      <td>1.000000</td>\n","      <td>0.680038</td>\n","      <td>-0.702305</td>\n","      <td>-0.907781</td>\n","      <td>-0.989413</td>\n","      <td>-0.987827</td>\n","      <td>-0.987057</td>\n","      <td>-0.987801</td>\n","      <td>-0.996334</td>\n","      <td>-0.989413</td>\n","      <td>-0.999876</td>\n","      <td>-0.989153</td>\n","      <td>-0.720891</td>\n","      <td>-1.000000</td>\n","      <td>-0.043398</td>\n","      <td>-0.257142</td>\n","      <td>-0.516341</td>\n","      <td>-0.995224</td>\n","      <td>-0.995417</td>\n","      <td>-0.995951</td>\n","      <td>-0.995346</td>\n","      <td>-0.995728</td>\n","      <td>-0.995224</td>\n","      <td>-0.999974</td>\n","      <td>-0.995305</td>\n","      <td>-0.955696</td>\n","      <td>-0.936508</td>\n","      <td>0.430891</td>\n","      <td>-0.138373</td>\n","      <td>-0.491604</td>\n","      <td>-0.036788</td>\n","      <td>-0.012892</td>\n","      <td>0.640011</td>\n","      <td>-0.485366</td>\n","      <td>-0.848947</td>\n","      <td>0.181907</td>\n","      <td>-0.040826</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.038758</td>\n","      <td>-0.002289</td>\n","      <td>-0.023863</td>\n","      <td>-0.998241</td>\n","      <td>-0.986774</td>\n","      <td>-0.993115</td>\n","      <td>-0.998216</td>\n","      <td>-0.986479</td>\n","      <td>-0.993825</td>\n","      <td>-0.801982</td>\n","      <td>-0.746505</td>\n","      <td>-0.717858</td>\n","      <td>0.838581</td>\n","      <td>0.705854</td>\n","      <td>0.673208</td>\n","      <td>-0.995057</td>\n","      <td>-0.999992</td>\n","      <td>-0.999882</td>\n","      <td>-0.999908</td>\n","      <td>-0.997772</td>\n","      <td>-0.987726</td>\n","      <td>-0.995109</td>\n","      <td>-0.836774</td>\n","      <td>-0.589200</td>\n","      <td>-0.773771</td>\n","      <td>0.312105</td>\n","      <td>-0.095254</td>\n","      <td>0.194399</td>\n","      <td>-0.007998</td>\n","      <td>0.266740</td>\n","      <td>-0.318965</td>\n","      <td>0.409731</td>\n","      <td>-0.224589</td>\n","      <td>0.520354</td>\n","      <td>-0.319167</td>\n","      <td>0.234376</td>\n","      <td>-0.102650</td>\n","      <td>-0.154974</td>\n","      <td>-0.189796</td>\n","      <td>0.598515</td>\n","      <td>...</td>\n","      <td>-0.999941</td>\n","      <td>-0.994542</td>\n","      <td>-1.0</td>\n","      <td>-1.000000</td>\n","      <td>0.560592</td>\n","      <td>-0.529957</td>\n","      <td>-0.857124</td>\n","      <td>-0.991433</td>\n","      <td>-0.989051</td>\n","      <td>-0.987932</td>\n","      <td>-0.992145</td>\n","      <td>-0.998404</td>\n","      <td>-0.991433</td>\n","      <td>-0.999902</td>\n","      <td>-0.989339</td>\n","      <td>-0.763372</td>\n","      <td>-0.897436</td>\n","      <td>-0.270529</td>\n","      <td>-0.539596</td>\n","      <td>-0.833661</td>\n","      <td>-0.995096</td>\n","      <td>-0.995645</td>\n","      <td>-0.995508</td>\n","      <td>-0.995683</td>\n","      <td>-0.997414</td>\n","      <td>-0.995096</td>\n","      <td>-0.999974</td>\n","      <td>-0.995566</td>\n","      <td>-1.000000</td>\n","      <td>-0.936508</td>\n","      <td>0.137735</td>\n","      <td>-0.366214</td>\n","      <td>-0.702490</td>\n","      <td>0.123320</td>\n","      <td>0.122542</td>\n","      <td>0.693578</td>\n","      <td>-0.615971</td>\n","      <td>-0.848164</td>\n","      <td>0.185124</td>\n","      <td>-0.037080</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 561 columns</p>\n","</div>"],"text/plain":["        0         1         2    ...       558       559       560\n","0  0.043580 -0.005970 -0.035054  ... -0.841559  0.179913 -0.051718\n","1  0.039480 -0.002131 -0.029067  ... -0.845092  0.180261 -0.047436\n","2  0.039978 -0.005153 -0.022651  ... -0.849230  0.180610 -0.042271\n","3  0.039785 -0.011809 -0.028916  ... -0.848947  0.181907 -0.040826\n","4  0.038758 -0.002289 -0.023863  ... -0.848164  0.185124 -0.037080\n","\n","[5 rows x 561 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"D2ZctHAxIhpp","executionInfo":{"status":"ok","timestamp":1621017264194,"user_tz":420,"elapsed":515,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}},"outputId":"1d4d0dfd-d49d-4bd1-97e2-b828eaca21e5"},"source":["# Read the target values into `y`\n","y = pd.read_csv(Path(\"target.csv\"))\n","y.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>activity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>standing</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>standing</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>standing</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>standing</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>standing</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   activity\n","0  standing\n","1  standing\n","2  standing\n","3  standing\n","4  standing"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HwXylHZ7Ihpp","executionInfo":{"status":"ok","timestamp":1621017269993,"user_tz":420,"elapsed":595,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}},"outputId":"6fc5baff-e8d9-4d64-e664-5eed52749051"},"source":["y.activity.value_counts()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["standing              1423\n","laying                1413\n","sitting               1293\n","walking               1226\n","walking_upstairs      1073\n","walking_downstairs     987\n","stand_to_lie            90\n","sit_to_lie              75\n","lie_to_sit              60\n","lie_to_stand            57\n","stand_to_sit            47\n","sit_to_stand            23\n","Name: activity, dtype: int64"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"L__f9GYZIhpq","executionInfo":{"status":"ok","timestamp":1621017315988,"user_tz":420,"elapsed":634,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}}},"source":["# Split the dataset into training and testing data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"gOZJO3GmIhpq","executionInfo":{"status":"ok","timestamp":1621017347490,"user_tz":420,"elapsed":532,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}}},"source":["# Scale the training and testing input features using StandardScaler\n","X_scaler = StandardScaler()\n","X_scaler.fit(X_train)\n","\n","X_train_scaled = X_scaler.transform(X_train)\n","X_test_scaled = X_scaler.transform(X_test)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9CkkMQRGIhpq","executionInfo":{"status":"ok","timestamp":1621017396350,"user_tz":420,"elapsed":523,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}},"outputId":"0b98947d-20a5-450a-f72b-af43d4d1aa92"},"source":["# Apply One-hot encoding to the target labels\n","enc = OneHotEncoder()\n","enc.fit(y_train)\n","\n","encoded_y_train = enc.transform(y_train).toarray()\n","encoded_y_test = enc.transform(y_test).toarray()\n","encoded_y_train[0]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"sEb-30gfIhpq"},"source":["# Build a Deep Neural Network"]},{"cell_type":"code","metadata":{"id":"HJ8tc3VKIhpq","executionInfo":{"status":"ok","timestamp":1621017414986,"user_tz":420,"elapsed":504,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}}},"source":["# Create a sequential model\n","model = Sequential()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zg-gmEI-Ihpq","executionInfo":{"status":"ok","timestamp":1621017484093,"user_tz":420,"elapsed":575,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}}},"source":["# Add the first layer where the input dimensions are the 561 columns of the training data\n","model.add(Dense(100, activation='relu', input_dim=X_train_scaled.shape[1]))"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-Ic3GmpIhpq","executionInfo":{"status":"ok","timestamp":1621017486243,"user_tz":420,"elapsed":521,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}}},"source":["# The output layer has 12 columns that are one-hot encoded\n","y_train.activity.value_counts()\n","number_outputs = 12"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"meoRwwh_Ihpq","executionInfo":{"status":"ok","timestamp":1621017512468,"user_tz":420,"elapsed":543,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}}},"source":["# Add output layer using 12 output nodes. \n","# HINT: Use `softmax` as the activation \n","model.add(Dense(number_outputs, activation=\"softmax\"))"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xhintm3Ihpr","executionInfo":{"status":"ok","timestamp":1621017580222,"user_tz":420,"elapsed":530,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}}},"source":["# Compile the model using categorical_crossentropy for the loss function, the adam optimizer,\n","# and add accuracy to the training metrics\n","model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qdDrtqXeIhpr","executionInfo":{"status":"ok","timestamp":1621017601987,"user_tz":420,"elapsed":552,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}},"outputId":"4c20fe20-231d-4709-ef17-7b71ff4ff1fc"},"source":["# Print the model summary\n","model.summary()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 100)               56200     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 12)                1212      \n","=================================================================\n","Total params: 57,412\n","Trainable params: 57,412\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZyJUMxsmIhpr","executionInfo":{"status":"ok","timestamp":1621017705906,"user_tz":420,"elapsed":7247,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}},"outputId":"f87e7294-9103-4f1c-a079-f9367243fed3"},"source":["# Use the training data to fit (train) the model\n","# @NOTE: Experiment with the number of training epochs to find the minimum iterations required to achieve a good accuracy\n","model.fit(\n","    X_train_scaled,\n","    encoded_y_train,\n","    epochs=30,\n","    shuffle=True,\n","    verbose=2\n",")"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","183/183 - 1s - loss: 0.3846 - accuracy: 0.8625\n","Epoch 2/30\n","183/183 - 0s - loss: 0.1242 - accuracy: 0.9567\n","Epoch 3/30\n","183/183 - 0s - loss: 0.0907 - accuracy: 0.9670\n","Epoch 4/30\n","183/183 - 0s - loss: 0.0680 - accuracy: 0.9756\n","Epoch 5/30\n","183/183 - 0s - loss: 0.0565 - accuracy: 0.9794\n","Epoch 6/30\n","183/183 - 0s - loss: 0.0470 - accuracy: 0.9839\n","Epoch 7/30\n","183/183 - 0s - loss: 0.0478 - accuracy: 0.9830\n","Epoch 8/30\n","183/183 - 0s - loss: 0.0387 - accuracy: 0.9863\n","Epoch 9/30\n","183/183 - 0s - loss: 0.0334 - accuracy: 0.9882\n","Epoch 10/30\n","183/183 - 0s - loss: 0.0284 - accuracy: 0.9904\n","Epoch 11/30\n","183/183 - 0s - loss: 0.0235 - accuracy: 0.9919\n","Epoch 12/30\n","183/183 - 0s - loss: 0.0224 - accuracy: 0.9930\n","Epoch 13/30\n","183/183 - 0s - loss: 0.0180 - accuracy: 0.9947\n","Epoch 14/30\n","183/183 - 0s - loss: 0.0204 - accuracy: 0.9930\n","Epoch 15/30\n","183/183 - 0s - loss: 0.0166 - accuracy: 0.9945\n","Epoch 16/30\n","183/183 - 0s - loss: 0.0151 - accuracy: 0.9948\n","Epoch 17/30\n","183/183 - 0s - loss: 0.0108 - accuracy: 0.9966\n","Epoch 18/30\n","183/183 - 0s - loss: 0.0137 - accuracy: 0.9945\n","Epoch 19/30\n","183/183 - 0s - loss: 0.0105 - accuracy: 0.9971\n","Epoch 20/30\n","183/183 - 0s - loss: 0.0104 - accuracy: 0.9962\n","Epoch 21/30\n","183/183 - 0s - loss: 0.0067 - accuracy: 0.9986\n","Epoch 22/30\n","183/183 - 0s - loss: 0.0063 - accuracy: 0.9983\n","Epoch 23/30\n","183/183 - 0s - loss: 0.0440 - accuracy: 0.9880\n","Epoch 24/30\n","183/183 - 0s - loss: 0.0512 - accuracy: 0.9847\n","Epoch 25/30\n","183/183 - 0s - loss: 0.0240 - accuracy: 0.9935\n","Epoch 26/30\n","183/183 - 0s - loss: 0.0139 - accuracy: 0.9967\n","Epoch 27/30\n","183/183 - 0s - loss: 0.0039 - accuracy: 0.9990\n","Epoch 28/30\n","183/183 - 0s - loss: 0.0022 - accuracy: 0.9998\n","Epoch 29/30\n","183/183 - 0s - loss: 0.0023 - accuracy: 1.0000\n","Epoch 30/30\n","183/183 - 0s - loss: 0.0011 - accuracy: 1.0000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f76cde98b50>"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"g8oweePFIhpr"},"source":["# Evaluate the Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7NXkzHVIhpr","executionInfo":{"status":"ok","timestamp":1621017735942,"user_tz":420,"elapsed":511,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}},"outputId":"8412b242-36a1-43f4-e238-be0a212dbcee"},"source":["# Evaluate the model using the testing data\n","model_loss, model_accuracy = model.evaluate(X_test_scaled, encoded_y_test, verbose=2)\n","print(f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"],"execution_count":17,"outputs":[{"output_type":"stream","text":["61/61 - 0s - loss: 0.0715 - accuracy: 0.9794\n","Normal Neural Network - Loss: 0.07145874947309494, Accuracy: 0.9794026613235474\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"J8R_2pC7Ihpr","executionInfo":{"status":"ok","timestamp":1621017741724,"user_tz":420,"elapsed":521,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}},"outputId":"177efa87-9cf5-4d57-dd92-cea4ce4af0e6"},"source":["# Make predictions\n","predicted = model.predict(X_test_scaled)\n","predicted = enc.inverse_transform(predicted).flatten().tolist()\n","results = pd.DataFrame({\n","    \"Actual\": y_test.activity.values,\n","    \"Predicted\": predicted\n","})\n","results.head(10)"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Actual</th>\n","      <th>Predicted</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>walking_upstairs</td>\n","      <td>walking_upstairs</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>laying</td>\n","      <td>laying</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>sitting</td>\n","      <td>sitting</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>sitting</td>\n","      <td>sitting</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>walking</td>\n","      <td>walking</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>sitting</td>\n","      <td>sitting</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>lie_to_sit</td>\n","      <td>lie_to_sit</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>walking_downstairs</td>\n","      <td>walking_downstairs</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>laying</td>\n","      <td>laying</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>sitting</td>\n","      <td>sitting</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Actual           Predicted\n","0    walking_upstairs    walking_upstairs\n","1              laying              laying\n","2             sitting             sitting\n","3             sitting             sitting\n","4             walking             walking\n","5             sitting             sitting\n","6          lie_to_sit          lie_to_sit\n","7  walking_downstairs  walking_downstairs\n","8              laying              laying\n","9             sitting             sitting"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h5vzROPzIhpr","executionInfo":{"status":"ok","timestamp":1621017752214,"user_tz":420,"elapsed":529,"user":{"displayName":"somya panda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPG3HPtIinE4WxzphAj532IXowQAFLSEzeDJd-Yw=s64","userId":"15898368594235905975"}},"outputId":"9155cd0f-5b1b-49d0-cb96-bcd5529eafc8"},"source":["# Print the Classification Report\n","from sklearn.metrics import classification_report\n","print(classification_report(results.Actual, results.Predicted))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["                    precision    recall  f1-score   support\n","\n","            laying       1.00      1.00      1.00       355\n","        lie_to_sit       0.88      0.93      0.90        15\n","      lie_to_stand       0.90      0.82      0.86        11\n","        sit_to_lie       0.90      0.83      0.86        23\n","      sit_to_stand       1.00      1.00      1.00         4\n","           sitting       0.98      0.95      0.97       337\n","      stand_to_lie       0.78      0.78      0.78        18\n","      stand_to_sit       0.88      0.93      0.90        15\n","          standing       0.96      0.98      0.97       367\n","           walking       0.99      0.99      0.99       300\n","walking_downstairs       0.99      0.99      0.99       230\n","  walking_upstairs       0.99      1.00      0.99       267\n","\n","          accuracy                           0.98      1942\n","         macro avg       0.94      0.93      0.93      1942\n","      weighted avg       0.98      0.98      0.98      1942\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FtMsMn0UIhpr"},"source":[""],"execution_count":null,"outputs":[]}]}